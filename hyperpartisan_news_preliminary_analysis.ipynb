{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repository will contain preliminary research on the hyperpartisan news detection\n",
    "\n",
    "https://www.aclweb.org/anthology/S19-2184/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper code: https://github.com/HLTCHKUST/hyperpartisan-news-detection/blob/master/utils/data_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "from gensim.corpora import Dictionary\n",
    "from data_utils import parse_xml, clean_txt\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parse_xml is good enough to parse data from the xml files. But we need a different parser for the ground truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml_GT(article):\n",
    "    \n",
    "    # get meta data\n",
    "    id = article.get('id')\n",
    "    label = article.get('hyperpartisan')\n",
    "    by = article.get('labeled-by')\n",
    "    url = article.get('url')\n",
    "    \n",
    "    result = [id, {'hyperpartisan':label,'labeled-by':by, 'url':url}]\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataframe(data_path, label_path):\n",
    "    \n",
    "    ## For data:\n",
    "    \n",
    "    data_xml_file = open(data_path).read()\n",
    "    data_soup = Soup(data_xml_file)\n",
    "    data_articles=data_soup.find_all('article')\n",
    "    #print(\"Number of articles: \", len(articles))\n",
    "    \n",
    "    data_preprocessed_dict={}\n",
    "    for a in tqdm(data_articles):\n",
    "        id, article = parse_xml(a)\n",
    "        data_preprocessed_dict[id] = article\n",
    "    \n",
    "    ##For labels or ground truths\n",
    "    GT_xml_file = open(label_path).read()\n",
    "    GT_soup = Soup(GT_xml_file)\n",
    "    GT_articles=GT_soup.find_all('article')\n",
    "    #print(\"Number of articles: \", len(articles))\n",
    "    \n",
    "    GT_preprocessed_dict={}\n",
    "    for a in tqdm(GT_articles):\n",
    "        id, article = parse_xml_GT(a)\n",
    "        GT_preprocessed_dict[id] = article\n",
    "        \n",
    "    \n",
    "        \n",
    "    df_data, df_labels = (pd.DataFrame.from_dict(data_preprocessed_dict).T, pd.DataFrame.from_dict(GT_preprocessed_dict).T)\n",
    "    \n",
    "    if(all(df_data.index==df_labels.index)):\n",
    "        return pd.concat([df_data,df_labels], 1)\n",
    "    else:\n",
    "        print(\"check the data and labels for possibel mismatch\")\n",
    "        return \"data and label mismatch\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/disk2/sadat/FakeNewsData/Hyperpartisan_news_2019_semeval/articles-training-byarticle-20181122.xml\"\n",
    "label_path = \"/disk2/sadat/FakeNewsData/Hyperpartisan_news_2019_semeval/ground-truth-training-byarticle-20181122.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645/645 [00:00<00:00, 740.83it/s]\n",
      "100%|██████████| 645/645 [00:00<00:00, 466274.75it/s]\n"
     ]
    }
   ],
   "source": [
    "df_Train_byarticle = build_dataframe(data_path, label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>internal</th>\n",
       "      <th>external</th>\n",
       "      <th>article_text</th>\n",
       "      <th>hyperpartisan</th>\n",
       "      <th>labeled-by</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000000</th>\n",
       "      <td>2017-09-10</td>\n",
       "      <td>Kucinich: Reclaiming the money power</td>\n",
       "      <td>4</td>\n",
       "      <td>[https://farm8.static.flickr.com/7020/65515348...</td>\n",
       "      <td>from flickr.com: money {mid numberplaceholder}...</td>\n",
       "      <td>true</td>\n",
       "      <td>article</td>\n",
       "      <td>https://www.opednews.com/articles/Kucinich-Rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000001</th>\n",
       "      <td>2017-10-12</td>\n",
       "      <td>Trump Just Woke Up &amp; Viciously Attacked Puerto...</td>\n",
       "      <td>0</td>\n",
       "      <td>[http://www.cnn.com/2017/03/16/politics/trump-...</td>\n",
       "      <td>donald trump ran on many braggadocios and larg...</td>\n",
       "      <td>true</td>\n",
       "      <td>article</td>\n",
       "      <td>http://bipartisanreport.com/2017/10/12/trump-j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000002</th>\n",
       "      <td>2017-10-11</td>\n",
       "      <td>Liberals wailing about gun control, but what a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>photo by justin images in response to joyce ne...</td>\n",
       "      <td>true</td>\n",
       "      <td>article</td>\n",
       "      <td>https://www.reviewjournal.com/opinion/letters/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000003</th>\n",
       "      <td>2017-09-24</td>\n",
       "      <td>Laremy Tunsil joins NFL players in kneeling du...</td>\n",
       "      <td>0</td>\n",
       "      <td>[https://twitter.com/UncleChaps/status/9119271...</td>\n",
       "      <td>after colin kaepernick rightly chose to kneel ...</td>\n",
       "      <td>true</td>\n",
       "      <td>article</td>\n",
       "      <td>https://www.redcuprebellion.com/2017/9/24/1635...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000004</th>\n",
       "      <td>2017-10-12</td>\n",
       "      <td>It's 1968 All Over Again</td>\n",
       "      <td>0</td>\n",
       "      <td>[http://www.nationalreview.com/redirect/amazon...</td>\n",
       "      <td>almost a half century ago, in numberplaceholde...</td>\n",
       "      <td>false</td>\n",
       "      <td>article</td>\n",
       "      <td>https://www.realclearpolitics.com/articles/201...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date                                              title  \\\n",
       "0000000  2017-09-10               Kucinich: Reclaiming the money power   \n",
       "0000001  2017-10-12  Trump Just Woke Up & Viciously Attacked Puerto...   \n",
       "0000002  2017-10-11  Liberals wailing about gun control, but what a...   \n",
       "0000003  2017-09-24  Laremy Tunsil joins NFL players in kneeling du...   \n",
       "0000004  2017-10-12                           It's 1968 All Over Again   \n",
       "\n",
       "        internal                                           external  \\\n",
       "0000000        4  [https://farm8.static.flickr.com/7020/65515348...   \n",
       "0000001        0  [http://www.cnn.com/2017/03/16/politics/trump-...   \n",
       "0000002        0                                                 []   \n",
       "0000003        0  [https://twitter.com/UncleChaps/status/9119271...   \n",
       "0000004        0  [http://www.nationalreview.com/redirect/amazon...   \n",
       "\n",
       "                                              article_text hyperpartisan  \\\n",
       "0000000  from flickr.com: money {mid numberplaceholder}...          true   \n",
       "0000001  donald trump ran on many braggadocios and larg...          true   \n",
       "0000002  photo by justin images in response to joyce ne...          true   \n",
       "0000003  after colin kaepernick rightly chose to kneel ...          true   \n",
       "0000004  almost a half century ago, in numberplaceholde...         false   \n",
       "\n",
       "        labeled-by                                                url  \n",
       "0000000    article  https://www.opednews.com/articles/Kucinich-Rec...  \n",
       "0000001    article  http://bipartisanreport.com/2017/10/12/trump-j...  \n",
       "0000002    article  https://www.reviewjournal.com/opinion/letters/...  \n",
       "0000003    article  https://www.redcuprebellion.com/2017/9/24/1635...  \n",
       "0000004    article  https://www.realclearpolitics.com/articles/201...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Train_byarticle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false    407\n",
       "true     238\n",
       "Name: hyperpartisan, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Train_byarticle.hyperpartisan.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train_byarticle['label'] = df_Train_byarticle['hyperpartisan'].apply(lambda x:1 if x=='true' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as ppb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pretrained BERT model\n",
    "# For DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## Want BERT instead of distilBERT? Uncomment the following line:\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_and_masking(df, col_name, maxlen=200, overlap=50):\n",
    "    maximum_word_length = max([len(L.split()) for L in list(df_Train_byarticle.title)])\n",
    "    if maximum_word_length<maxlen:\n",
    "        tokenized = df[col_name].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "            # Padding: The token list have difference in size. So, let's put them in the same length\n",
    "                #tokenized is Dataframe --the dataset that has the tokenized values\n",
    "        max_len = 0 #for padding\n",
    "        for i in tokenized.values:\n",
    "          if len(i) > max_len:\n",
    "              max_len = len(i)\n",
    "\n",
    "        padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "    \n",
    "        attention_mask = np.where(padded != 0, 1, 0) #np.where(condition, value that will be true if condition is True, value returned if false)\n",
    "        \n",
    "    return padded, attention_mask\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_counts(LIST, maxlen, overlap):\n",
    "    \n",
    "    '''With a given list of numbers, the maximum length it can be, and what is the overlap,\n",
    "    The code converts them in sliced lists.\n",
    "    Example: Z = make_counts([1,2,3,4,5,6,7,8,9,10], maxlen=4, overlap=2)\n",
    "    Z = [[1, 2, 3, 4], [3, 4, 5, 6], [5, 6, 7, 8], [7, 8, 9, 10]]'''\n",
    "    \n",
    "    \n",
    "    start = 0\n",
    "    finish = start+maxlen\n",
    "    listlen = len(LIST)\n",
    "    idx = []\n",
    "    while(start<listlen):\n",
    "\n",
    "      if finish<listlen:\n",
    "        idx.append([start, finish])\n",
    "        start = finish-overlap\n",
    "        finish = start+maxlen\n",
    "      else:\n",
    "        finish = listlen\n",
    "        idx.append([start, finish])\n",
    "        break\n",
    "        \n",
    "    sliced_list = []\n",
    "    for i in idx:\n",
    "        sliced_list.append(LIST[i[0]:i[1]])\n",
    "        \n",
    "        \n",
    "    return sliced_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_and_masking_v2(df, col_name, maxlen=200, overlap=50):\n",
    "    \n",
    "    '''If the length of the tokenized values are more than 200, https://arxiv.org/abs/1910.10781 this paper suggests\n",
    "    to break it down to 200 lengthed tokens'''\n",
    "    \n",
    "    df1 = pd.DataFrame()\n",
    "    df1[\"tokenized\"] = df[col_name].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "        # Padding: The token list have difference in size. So, let's put them in the same length\n",
    "            #tokenized is Dataframe --the dataset that has the tokenized values\n",
    "    \n",
    "    \n",
    "    \n",
    "    df1[\"splitted_tokens\"] = df1[\"tokenized\"].apply(lambda x:make_counts(x, maxlen, overlap))\n",
    "    df1[\"count\"] = df1[\"splitted_tokens\"].apply(lambda x:len(x))\n",
    "    df1[\"padded_tokens\"] = df1[\"splitted_tokens\"].apply(lambda x:[i + [0]*(maxlen-len(i)) for i in x])\n",
    "    padded = np.array([item for sublist in list(df1[\"padded_tokens\"]) for item in sublist])\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "    return df1, padded, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's work for the titles only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title, padded_title, attention_mask_title = create_padding_and_masking_v2(df_Train_byarticle, \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(645, 200)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_hidden_state(padded, attention_mask):\n",
    "    '''Given the padded value and the attention mask, the pre-trained BERT (distillBERT) model\n",
    "    will give the required embedding '''\n",
    "    \n",
    "    start_time = time.time()\n",
    "    input_ids = torch.tensor(padded)  \n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    time_lapsed = time.time()-start_time\n",
    "    print(time_lapsed)\n",
    "    return last_hidden_states\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.6024956703186\n"
     ]
    }
   ],
   "source": [
    "last_hidden_states_title = find_last_hidden_state(padded_title, attention_mask_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_title = last_hidden_states_title[0][:,0,:] # We are interested in the [cls] tokken only\n",
    "features_title = features_title.numpy() # features_title is a tensor, we need to convert it to numpy\n",
    "label = df_Train_byarticle.label.to_numpy() # And so is the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run models for titles only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = features_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy percentage for Log reg is 0.688533653846154\n",
      "Average accuracy percentage for SVM is 0.6905048076923077\n"
     ]
    }
   ],
   "source": [
    "score_lr = []\n",
    "score_SVM = []\n",
    "for train_index, test_index in kf.split(data):\n",
    "    train_X = data[train_index]\n",
    "    train_y = label[train_index]\n",
    "    test_X = data[test_index]\n",
    "    test_y = label[test_index]\n",
    "    lr_clf = LogisticRegression()\n",
    "    svm_clf = sklearn.svm.SVC()\n",
    "    lr_clf.fit(train_X, train_y)\n",
    "    svm_clf.fit(train_X, train_y)\n",
    "    prediction_lr = lr_clf.predict(test_X)\n",
    "    prediction_svm = svm_clf.predict(test_X)\n",
    "    score_lr.append(sklearn.metrics.accuracy_score(test_y, prediction_lr))\n",
    "    score_SVM.append(sklearn.metrics.accuracy_score(test_y, prediction_svm))\n",
    "\n",
    "print(\"Average accuracy percentage for Log reg is \"+str(sum(score_lr)/len(score_lr)))\n",
    "print(\"Average accuracy percentage for SVM is \"+str(sum(score_SVM)/len(score_SVM)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's work for the articles only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1833 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "df_article, padded_article, attention_mask_article = create_padding_and_masking_v2(df_Train_byarticle, \"article_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3352, 200)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_article.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158.5051052570343\n"
     ]
    }
   ],
   "source": [
    "last_hidden_states_articles = find_last_hidden_state(padded_article, attention_mask_article)\n",
    "features_article = last_hidden_states_articles[0][:,0,:]\n",
    "features_article = features_article.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Averaging_long_text(L, features, feat_shape=768):\n",
    "    '''This function will be useful for the long texts, over 200 tokens only. We will average them'''\n",
    "    idx = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "\n",
    "    for i in L:\n",
    "      end = start + i\n",
    "      idx.append([start, end])\n",
    "      start = end \n",
    "    feat_avg = np.zeros((len(idx), feat_shape))\n",
    "    J = 0\n",
    "    for index in idx:\n",
    "        feat_avg[J, 0:feat_shape] = np.mean(features[index[0]:index[1],:], 0)\n",
    "        J += 1\n",
    "    return feat_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_avg = Averaging_long_text(list(df_article[\"count\"]), features_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(645, 768)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_avg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = feat_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model for article text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy percentage for Log reg is 0.8001923076923078\n",
      "Average accuracy percentage for SVM is 0.8081009615384616\n"
     ]
    }
   ],
   "source": [
    "score_lr = []\n",
    "score_SVM = []\n",
    "for train_index, test_index in kf.split(data):\n",
    "    train_X = data[train_index]\n",
    "    train_y = label[train_index]\n",
    "    test_X = data[test_index]\n",
    "    test_y = label[test_index]\n",
    "    lr_clf = LogisticRegression()\n",
    "    svm_clf = sklearn.svm.SVC()\n",
    "    lr_clf.fit(train_X, train_y)\n",
    "    svm_clf.fit(train_X, train_y)\n",
    "    prediction_lr = lr_clf.predict(test_X)\n",
    "    prediction_svm = svm_clf.predict(test_X)\n",
    "    score_lr.append(sklearn.metrics.accuracy_score(test_y, prediction_lr))\n",
    "    score_SVM.append(sklearn.metrics.accuracy_score(test_y, prediction_svm))\n",
    "\n",
    "print(\"Average accuracy percentage for Log reg is \"+str(sum(score_lr)/len(score_lr)))\n",
    "print(\"Average accuracy percentage for SVM is \"+str(sum(score_SVM)/len(score_SVM)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we combine them? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will explore the idea of combining the titles and texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine 1**: A simple feature concatention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining features_title and feat_avg\n",
    "combined1 = np.concatenate((features_title, feat_avg), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(645, 1536)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combined1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy percentage for Log reg is 0.8034375\n",
      "Average accuracy percentage for SVM is 0.8080528846153847\n"
     ]
    }
   ],
   "source": [
    "score_lr = []\n",
    "score_SVM = []\n",
    "for train_index, test_index in kf.split(data):\n",
    "    train_X = data[train_index]\n",
    "    train_y = label[train_index]\n",
    "    test_X = data[test_index]\n",
    "    test_y = label[test_index]\n",
    "    lr_clf = LogisticRegression()\n",
    "    svm_clf = sklearn.svm.SVC()\n",
    "    lr_clf.fit(train_X, train_y)\n",
    "    svm_clf.fit(train_X, train_y)\n",
    "    prediction_lr = lr_clf.predict(test_X)\n",
    "    prediction_svm = svm_clf.predict(test_X)\n",
    "    score_lr.append(sklearn.metrics.accuracy_score(test_y, prediction_lr))\n",
    "    score_SVM.append(sklearn.metrics.accuracy_score(test_y, prediction_svm))\n",
    "\n",
    "print(\"Average accuracy percentage for Log reg is \"+str(sum(score_lr)/len(score_lr)))\n",
    "print(\"Average accuracy percentage for SVM is \"+str(sum(score_SVM)/len(score_SVM)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So, not much improvement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine2:** Let's average the output probability score and produce output based on that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(581, 1536)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(581, 768)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[train_index,0:768].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy percentage for Log reg is 0.7957451923076924\n"
     ]
    }
   ],
   "source": [
    "score_lr = []\n",
    "score_SVM = []\n",
    "for train_index, test_index in kf.split(data):\n",
    "    \n",
    "    train_X_title = data[train_index, 0:768]\n",
    "    train_y = label[train_index]\n",
    "    test_X_title = data[test_index, 0:768]\n",
    "    test_y = label[test_index]\n",
    "    \n",
    "    train_X_article = data[train_index, 768:1536]\n",
    "    test_X_article = data[test_index, 768:1536]\n",
    "    \n",
    "    lr_clf_title = LogisticRegression()\n",
    "    lr_clf_title.fit(train_X_title, train_y)\n",
    "    prediction_prob_lr_title = lr_clf_title.predict_proba(test_X_title)\n",
    "\n",
    "    lr_clf_article = LogisticRegression()\n",
    "    lr_clf_article.fit(train_X_article, train_y)\n",
    "    prediction_prob_lr_article = lr_clf_article.predict_proba(test_X_article)    \n",
    "    \n",
    "    sum_pred_prob = prediction_prob_lr_title + prediction_prob_lr_article\n",
    "    prediction_lr = np.where(sum_pred_prob[:,1]>1, 1, 0)\n",
    "    score_lr.append(sklearn.metrics.accuracy_score(test_y, prediction_lr))\n",
    "\n",
    "print(\"Average accuracy percentage for Log reg is \"+str(sum(score_lr)/len(score_lr)))\n",
    "#print(\"Average accuracy percentage for SVM is \"+str(sum(score_SVM)/len(score_SVM)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No, the scores didnot improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
